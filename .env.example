# =============================================================================
# Environment Configuration
# =============================================================================

ENVIRONMENT=development  # development | staging | production
DEBUG=false
LOG_LEVEL=INFO  # DEBUG | INFO | WARNING | ERROR | CRITICAL

# =============================================================================
# Neo4j Database
# =============================================================================

# URI Formats:
# Local: bolt://localhost:7687
# Cloud (Aura): neo4j+s://<instance-id>.databases.neo4j.io
NEO4J_URI=bolt://localhost:7687

NEO4J_USER=neo4j
NEO4J_PASS=your_password_here
NEO4J_DB=neo4j

# Connection Pool Settings
NEO4J_MAX_CONNECTION_LIFETIME=3600
NEO4J_MAX_CONNECTION_POOL_SIZE=50
NEO4J_CONNECTION_TIMEOUT=30

# =============================================================================
# Embedding Configuration
# =============================================================================

# Provider: openai | ollama | sentence_transformer
EMBEDDING_PROVIDER=ollama

# Note: Dimension must match the specific model used (e.g., 1024 for mxbai, 384 for MiniLM, 1536 for OpenAI)
EMBEDDING_DIMENSION=1024
EMBEDDING_BATCH_SIZE=100
EMBEDDING_CACHE_TTL=3600

# OpenAI Settings
OPENAI_API_KEY=
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_MAX_RETRIES=3
OPENAI_TIMEOUT=60

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=mxbai-embed-large
OLLAMA_TIMEOUT=120

# Sentence Transformer Settings (Local CPU/GPU)
# Common model: sentence-transformers/all-MiniLM-L6-v2 (Requires EMBEDDING_DIMENSION=384)
SENTENCE_TRANSFORMER_MODEL=sentence-transformers/all-MiniLM-L6-v2
SENTENCE_TRANSFORMER_DEVICE=cpu  # cpu | cuda | mps

# =============================================================================
# LLM Configuration (for Question Answering)
# =============================================================================

# Provider: ollama | gemini
LLM_PROVIDER=ollama

# LLM General Settings
LLM_TIMEOUT=120
LLM_MAX_TOKENS=1000
LLM_TEMPERATURE=0.7

# Ollama LLM Settings
LLM_MODEL_OLLAMA=llama3.2

# Google Gemini Settings
GEMINI_API_KEY=your_gemini_api_key_here
LLM_MODEL_GEMINI=gemini-2.5-flash

# =============================================================================
# Vector Search Configuration
# =============================================================================

VECTOR_SEARCH_DEFAULT_LIMIT=10
VECTOR_SEARCH_MAX_LIMIT=100
HYBRID_SEARCH_VECTOR_WEIGHT=0.7  # 0.0 to 1.0
VECTOR_INDEX_SIMILARITY_FUNCTION=cosine  # cosine | euclidean

# =============================================================================
# API Configuration
# =============================================================================

API_HOST=0.0.0.0
API_PORT=8000
API_VERSION=v1
API_TITLE=Secrin API
API_DESCRIPTION=Code analysis and vector search API
API_CORS_ORIGINS=["*"]
API_RATE_LIMIT_PER_MINUTE=60

# =============================================================================
# Performance & Optimization
# =============================================================================

QUERY_CACHE_SIZE=1000
QUERY_CACHE_TTL=300
MAX_WORKERS=4

# =============================================================================
# Observability & Monitoring
# =============================================================================

METRICS_ENABLED=false
METRICS_PORT=9090
TRACING_ENABLED=false
TRACING_ENDPOINT=

# =============================================================================
# Security
# =============================================================================

API_KEY=
ALLOWED_HOSTS=["*"]

# =============================================================================
# App Store Configuration
# =============================================================================

GITHUB_WEBHOOK_SECRET=

# =============================================================================
# Feature Flags (Environment-level overrides)
# Use these to override default feature flag behavior
# For fine-grained control, use the feature_flags.py system
# =============================================================================

FEATURE_EMBEDDINGS=true
FEATURE_VECTOR_SEARCH=true
FEATURE_HYBRID_SEARCH=true
FEATURE_BATCH_PROCESSING=true
FEATURE_CACHING=false
